{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#install the database GeoLiteCity to use pygeoip module\n",
    "#dependencies: pandas,numpy,seaborn,collections,tldextract,itertools,pygeoip,scipy,matplotlib,urllib,,sklearn,nltk,re,sys,pickle,sklearn,xgboost,lightgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tldextract\n",
    "from collections import defaultdict\n",
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "from scipy import signal\n",
    "from urllib.parse import urlparse \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas.io.parsers import ParserError\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import csv\n",
    "import io\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import DBSCAN \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import normalize\n",
    "import pygeoip\n",
    "import inspect\n",
    "import readline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "gip=pygeoip.GeoIP('C:\\\\Users\\\\RAPTOR\\\\Downloads\\\\GeoLiteCity.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def find_out(df):\n",
    "    to_drop=[]\n",
    "    pot=df.copy()\n",
    "    pot=pot.select_dtypes(include=['object'])\n",
    "    col_names=pot.columns.values.tolist()\n",
    "    for col_name in col_names:\n",
    "\n",
    "\n",
    "        make_list=df[col_name].unique().tolist()\n",
    "        car_list=list(itertools.permutations(df[col_name],2))\n",
    "\n",
    "        for i in make_list:\n",
    "            top=list(filter(lambda x:x[0]==i,car_list))\n",
    "            top=top[:len(df)-1]\n",
    "            res=defaultdict(list)\n",
    "            for i,j in top:\n",
    "                res[i].append(j)\n",
    "            uu=dict(res)\n",
    "\n",
    "            for i,j in uu.items():\n",
    "                cols=uu.get(i)\n",
    "                su=0\n",
    "                for k in cols:\n",
    "                    su=su+fuzz.partial_token_sort_ratio(i,k)\n",
    "                su=su+100\n",
    "                su=su/len(df)\n",
    "\n",
    "                pot[col_name]=pot[col_name].replace(i,su)\n",
    "                \n",
    "    df2=pd.get_dummies(df)\n",
    "    #print(pot.head())\n",
    "    for l in col_names:\n",
    "        #print(l)\n",
    "        f=pot[l]\n",
    "        f_Q1=f.quantile(0.25)\n",
    "        f_Q3=f.quantile(0.75)\n",
    "        f_IQR=f_Q3-f_Q1\n",
    "        f_lowerend=f_Q1-(1.5*f_IQR)\n",
    "        f_upperend=f_Q3+(1.5*f_IQR)\n",
    "        f_outliers=f[(f<f_lowerend) | (f>f_upperend)]\n",
    "        list_outliers=f_outliers.index.values.tolist()\n",
    "   \n",
    "        real_outlier=[]\n",
    "        \n",
    "    \n",
    "    #real_outlier is the list of names of outlier \n",
    "        for out in list_outliers:\n",
    "        \n",
    "            real_outlier.append(df[l].iloc[out])\n",
    "        \n",
    "        real_outlier=list(set(real_outlier))\n",
    "    #print(real_outlier)\n",
    "    \n",
    "        if real_outlier!=None:\n",
    "        \n",
    "        \n",
    "            for j in real_outlier:\n",
    "            #print(j)\n",
    "                mac=str(l)+\"_\"+str(j)\n",
    "                #print(mac)\n",
    "                value=df2.corr()[mac].values.tolist()\n",
    "                values=np.nansum(value)\n",
    "                \n",
    "                \n",
    "                if -1< values <1:\n",
    "                    new=df[df[l]==j].index.values.tolist()\n",
    "                    \n",
    "                    to_drop=new + to_drop\n",
    "                    \n",
    "    #to_drop=[a for a in to_drop]\n",
    "    #print(to_drop)\n",
    "    df=df.drop(df.index[to_drop])\n",
    "    return df\n",
    "\"\"\"\n",
    "#function for one-hot encoding\n",
    "#var is the variable to be encoded and cat is a dataframe\n",
    "def one_hot(var,cat):\n",
    "    \n",
    "                        \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    cat = cat.apply(le.fit_transform)\n",
    "                   \n",
    "                    #one_hot_encoding\n",
    "                    \n",
    "\n",
    "    \n",
    "    clf = OneHotEncoder()\n",
    "                    #print(cat_df[col_name[i]])\n",
    "\n",
    "    X1 = clf.fit_transform(var.values.reshape(-1,1)).toarray()\n",
    "    return X1\n",
    "#function to get country and city to a dataframe\n",
    "#df is the the dataframe and i is the feature\n",
    "def country_city(i,df):\n",
    "    df['country_'+str(i)]= df[i].apply(check_country)\n",
    "    df['city_'+str(i)]=df[i].apply(check_city)\n",
    "    df=df.drop([i],axis=1)\n",
    "    return df\n",
    "#returns the domain\n",
    "def check_domain(url):\n",
    "    ext=tldextract.extract(url)\n",
    "    if(re.findall(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',str(url))):\n",
    "        return np.nan\n",
    "    elif(ext.domain==''):\n",
    "        return np.nan\n",
    "    else:\n",
    "        \n",
    "        return ext.domain\n",
    "    \n",
    "\n",
    "#returns the subdomain\n",
    "def check_subdomain(url):\n",
    "    ext=tldextract.extract(url)\n",
    "    if(re.findall(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',str(url))):\n",
    "        return np.nan\n",
    "    elif(ext.subdomain==''):\n",
    "        return np.nan\n",
    "    elif(ext.subdomain=='www'):\n",
    "        o=urlparse(url)\n",
    "        if(o.scheme==''):\n",
    "            return np.nan\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        return ext.subdomain\n",
    "\n",
    "\n",
    "#returns the tld\n",
    "def check_suffix(url):\n",
    "    ext=tldextract.extract(url)\n",
    "    if(re.findall(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',str(url))):\n",
    "        return np.nan\n",
    "    elif(ext.suffix==''):\n",
    "        return np.nan\n",
    "    else:\n",
    "        \n",
    "        return ext.suffix\n",
    "    \n",
    "    \n",
    "\n",
    "#returns the protocol\n",
    "def check_scheme(url):\n",
    "    o=urlparse(url)\n",
    "    if(o.scheme==''):\n",
    "        return np.nan\n",
    "    else:\n",
    "        \n",
    "        return o.scheme\n",
    "\n",
    "#returns email domain\n",
    "def check_email_domain(email):\n",
    "    return email.split('@')[1]\n",
    "#function for datetime features\n",
    "#i is the datetime feature and df is the dataframe\n",
    "def date_time(i,df):\n",
    "    df[i]=pd.to_datetime(df[i])\n",
    "    #df['weekend']=[0 if df[i].dt.dayofweek<5 else 1]\n",
    "    df['weekday']=df[i].apply(check_weekday)\n",
    "    df['year_'+str(i)]=df[i].dt.year\n",
    "    df['month_'+str(i)]=df[i].dt.month\n",
    "    dateLastTrain = pd.DataFrame({'Date':np.repeat(date.today(),[len(df)]) })\n",
    "    dateLastTrain['Date']=pd.to_datetime(dateLastTrain['Date'])\n",
    "    df['OpenAges_'+str(i)]=dateLastTrain['Date'].dt.year - df[i].dt.year\n",
    "    df=df.drop([i],axis=1)\n",
    "    return df\n",
    "\n",
    "# for standardizating,train is the dataset\n",
    "def stand_norm(train):\n",
    "    \n",
    "            \n",
    "    scaler = StandardScaler() \n",
    "    X_scaled = scaler.fit_transform(train) \n",
    "\n",
    "            \n",
    "    X_normalized = normalize(X_scaled)\n",
    "    return X_normalized\n",
    "#function to tokenize a sentence\n",
    "def regex_token(df,var):\n",
    "    token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "                    \n",
    "    cv = TfidfVectorizer(lowercase=True,stop_words='english',tokenizer = token.tokenize)\n",
    "                    \n",
    "    text_counts= cv.fit_transform(df[var].values)\n",
    "                    \n",
    "    dfOneHot =pd.DataFrame(columns=cv.get_feature_names(),data=text_counts.toarray())\n",
    "    return dfOneHot\n",
    "\n",
    "#returns country name and city from the given ip\n",
    "def check_country(tt):\n",
    "    \n",
    "    res=gip.record_by_addr(tt)\n",
    "    if res==None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        \n",
    "        return res['country_name']\n",
    "    \n",
    "def check_city(tt):\n",
    "    res=gip.record_by_addr(tt)\n",
    "    if res==None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        \n",
    "        return res['city']\n",
    "    \n",
    "\n",
    "\n",
    "#checks for weekday    \n",
    "def check_weekday(tt):\n",
    "    return 0 if tt.weekday()<5 else 1\n",
    "#adds domain,subdomain,suffix,protocol of a feature to the dataframe\n",
    "def ip_address(i,df):\n",
    "    df['domain_'+str(i)]=df[i].apply(check_domain)\n",
    "    df['subdomain_'+str(i)]=df[i].apply(check_subdomain)\n",
    "    df['suffix_'+str(i)]=df[i].apply(check_suffix)\n",
    "    df['protocol_'+str(i)]=df[i].apply(check_scheme)\n",
    "    df=df.drop([i],axis=1)\n",
    "    return df\n",
    "                                   \n",
    "\n",
    "#find n_components\n",
    "def get_pca(df):\n",
    "    try:\n",
    "        \n",
    "        pca = PCA().fit(df)\n",
    "        data=np.cumsum(pca.explained_variance_ratio_)\n",
    "        max_peakind = signal.find_peaks_cwt(data, np.arange(1,10))\n",
    "        pca_val=max(data[max_peakind])\n",
    "        n_com=list(data).index(pca_val)\n",
    "        return n_com\n",
    "    except Exception as e:\n",
    "        print(\"Handled here\")\n",
    "        return len(df.columns.values.tolist())\n",
    "\n",
    "#finds more features from the dataset like day,month ..etc from datetime,country,city from an IP address ..etc\n",
    "def find(df,f):\n",
    "    try:\n",
    "        cols=df.columns.values.tolist()\n",
    "        for i in cols:\n",
    "            xx=df[i][0]\n",
    "            #print(i)\n",
    "            #print(xx)\n",
    "            import re\n",
    "\n",
    "            #print(i)\n",
    "            r0=re.findall(r'^([0-1][0-9]|[2][0-3]):([0-5][0-9])',str(xx))\n",
    "            try:\n",
    "\n",
    "\n",
    "                if r0[0]!=None:\n",
    "                    \n",
    "                    f.write(\"#\"+str(i)+\" find to be a datetime object\\n\")\n",
    "                    f.write(\"\\n#Extracted hour from \"+str(i))\n",
    "\n",
    "\n",
    "\n",
    "                    df[i]=pd.to_datetime(df[i])\n",
    "                    df[str(i)+'hour']=df[i].dt.hour\n",
    "                        #print(cols[i])\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write('df[\"'+i+'\"]=pd.to_datetime(df[\"'+i+'\"])\\n')\n",
    "                    f.write('df[\"'+i+'hour\"]=df[\"'+i+'\"].dt.hour\\n')\n",
    "                    f.write('df=df.drop([\"'+i+'\"],axis=1)\\n')\n",
    "                    df=df.drop([i],axis=1)\n",
    "\n",
    "\n",
    "            except IndexError:\n",
    "\n",
    "\n",
    "                r1=re.findall(r'\\d{1,4}\\/\\d{1,2}\\/\\d{2,4}|\\d{1,4}\\-\\d{1,2}\\-\\d{2,4}|^\\w{3,8}[\\s,].\\d{1,4}|\\d{1,2}:\\d{1,2} [A,P]M',str(xx))\n",
    "                try:\n",
    "\n",
    "\n",
    "                    if r1[0]!=None:\n",
    "\n",
    "                        try:\n",
    "                            \n",
    "                            \n",
    "\n",
    "                            f.write(\"#\"+str(i)+\"find to be a datetime object\\n\")\n",
    "                    \n",
    "                            f.write(\"\\n#Extracted weekday,year,month,openAges from \"+str(i))\n",
    "                            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "                            df[i]=pd.to_datetime(df[i])\n",
    "                                        #df['weekend']=[0 if df[i].dt.dayofweek<5 else 1]\n",
    "                            df['weekday']=df[i].apply(check_weekday)\n",
    "                            df['year_'+str(i)]=df[i].dt.year\n",
    "                            df['month_'+str(i)]=df[i].dt.month\n",
    "                            dateLastTrain = pd.DataFrame({'Date':np.repeat(date.today(),[len(df)]) })\n",
    "                            dateLastTrain['Date']=pd.to_datetime(dateLastTrain['Date'])\n",
    "                            df['OpenAges_'+str(i)]=dateLastTrain['Date'].dt.year - df[i].dt.year\n",
    "                            lines=inspect.getsource(check_weekday)\n",
    "                            f.write(lines)\n",
    "                            f.write(\"\\n\")\n",
    "                            lines=inspect.getsource(date_time)\n",
    "                            f.write(lines)\n",
    "                            f.write(\"\\n\")\n",
    "                            f.write('df=date_time(\"'+i+'\",df)\\n')\n",
    "\n",
    "                        except Exception as inst:\n",
    "                            \"\"\"\n",
    "                            x, y = inst.args     \n",
    "                            drop_index=df[df[i]==y].index.item()\n",
    "                            df=df.drop(index=drop_index)\n",
    "                            \"\"\"\n",
    "                        try:\n",
    "\n",
    "                            df[str(i)+'_hour']=df[i].dt.hour\n",
    "                            df=df.drop([col_name[i]],axis=1)\n",
    "\n",
    "                        except Exception:\n",
    "\n",
    "                            continue\n",
    "\n",
    "\n",
    "\n",
    "                except IndexError:\n",
    "\n",
    "\n",
    "                    r2=re.findall(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',str(xx))\n",
    "                    try:\n",
    "\n",
    "                        if r2[0]!=None:\n",
    "                        \n",
    "                            f.write(\"#\"+str(i)+\" finds to be an ip address\\n\")\n",
    "                            f.write(\"\\n#Extracted country and city from \"+str(i))\n",
    "                            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "                            df['country_'+str(i)]= df[i].apply(check_country)\n",
    "                            df['city_'+str(i)]=df[i].apply(check_city)\n",
    "                            lines=inspect.getsource(check_country)\n",
    "                            f.write(lines)\n",
    "                            f.write(\"\\n\")\n",
    "                            lines=inspect.getsource(check_city)\n",
    "                            f.write(lines)\n",
    "                            f.write(\"\\n\")\n",
    "                            lines=inspect.getsource(country_city)\n",
    "                            f.write(lines)\n",
    "                            f.write(\"\\n\")\n",
    "                            f.write('df=country_city(\"'+i+'\",df)\\n')\n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    except IndexError:\n",
    "\n",
    "                        r3=re.findall(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})',str(xx))\n",
    "                        try:\n",
    "\n",
    "                            if r3[0]!=None:\n",
    "\n",
    "\n",
    "\n",
    "                                try:\n",
    "\n",
    "\n",
    "                                    f.write(\"#\"+str(i)+\" finds to be an url\\n\")\n",
    "                                    f.write(\"#Extracted domain,subdomain,suffix and protocol\\n\")\n",
    "                                    df['domain_'+str(i)]=df[i].apply(check_domain)\n",
    "                                    df['subdomain_'+str(i)]=df[i].apply(check_subdomain)\n",
    "                                    df['suffix_'+str(i)]=df[i].apply(check_suffix)\n",
    "                                    df['protocol_'+str(i)]=df[i].apply(check_scheme)\n",
    "                                    df=df.drop([i],axis=1)\n",
    "                                    lines=inspect.getsource(check_domain)\n",
    "                                    f.write(lines)\n",
    "                                    f.write(\"\\n\")\n",
    "                                    lines=inspect.getsource(check_subdomain)\n",
    "                                    f.write(lines)\n",
    "                                    f.write(\"\\n\")\n",
    "                                    lines=inspect.getsource(check_suffix)\n",
    "                                    f.write(lines)\n",
    "                                    f.write(\"\\n\")\n",
    "                                    lines=inspect.getsource(check_scheme)\n",
    "                                    f.write(lines)\n",
    "                                    f.write(\"\\n\")\n",
    "                                    lines=inspect.getsource(ip_address)\n",
    "                                    f.write(lines)\n",
    "                                    f.write(\"\\n\")\n",
    "                                    f.write('df=ip_address(\"'+i+'\",df)\\n')\n",
    "                                    \n",
    "\n",
    "                                except Exception as inst:\n",
    "\n",
    "                                    x, y = inst.args     \n",
    "\n",
    "\n",
    "                                    drop_index=df[df[i]==y].index.item()\n",
    "                                    df=df.drop(index=drop_index)\n",
    "\n",
    "\n",
    "\n",
    "                        except IndexError:\n",
    "                            r4=re.findall(r'^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})$',str(xx))\n",
    "                            try:\n",
    "                                if r4[0]!=None:\n",
    "                                    f.write(\"#\"+str(i)+\" finds to be an email address\\n\")\n",
    "                                    f.write('#Extracted email domain\\n')\n",
    "                                    \n",
    "\n",
    "                                    df['domain_'+str(i)]=df[i].apply(check_email_domain)\n",
    "                                    df=df.drop([i],axis=1)\n",
    "                                    lines=inspect.getsource(check_email_domain)\n",
    "                                    f.write(lines)\n",
    "                                    f.write(\"\\n\")\n",
    "                                    f.write('df[\"domain_'+i+'\"]=df[\"'+i+'\"].apply(check_email_domain)\\n')\n",
    "                                    f.write('df=df.drop([\"'+i+'\"],axis=1)\\n')\n",
    "\n",
    "\n",
    "                            except IndexError:\n",
    "                                continue\n",
    "\n",
    "\n",
    "            \n",
    "                            \n",
    "\n",
    "\n",
    "    except Exception as inst:\n",
    "        print(\"Ooopsss !!!!\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        sys.exit(inst)\n",
    "            \n",
    "    print(df.columns.values.tolist())\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#categorical encoding\n",
    "def cat_encoding(train,y,f):\n",
    "    \n",
    "    print(\"Starting Categorical encoding !\")\n",
    "    f.write(\"\\n#Starting Categorical Encoding\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        \n",
    "    \n",
    "        df=train.copy()\n",
    "        #imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        #df=pd.DataFrame(imp.fit_transform(df),columns=train.columns,index=train.index)\n",
    "        #df=df.fillna(df.mode()[0])\n",
    "        #df=df.where(pd.notna(df), df.mode(), axis='columns')\n",
    "       \n",
    "        #null_val_perc(df)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #train=train.dropna()\n",
    "\n",
    "        #df=train.copy()\n",
    "        #print(df.head(12))\n",
    "        #excludes datetime features from categorical encoding if any \n",
    "        excpt=df.select_dtypes(include=['datetime64'])\n",
    "        \n",
    "        df=df.select_dtypes(include=['object','number'])\n",
    "       \n",
    "        \n",
    "\n",
    "        \n",
    "        cat_clos=df.select_dtypes(exclude=[np.number]).columns.values.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        col_name=[]\n",
    "        col_name=df.columns\n",
    "        new_df=pd.DataFrame()\n",
    "        for i in range(len(col_name)):\n",
    "            cat_df=pd.DataFrame()\n",
    "            if(((col_name[i] in cat_clos))):\n",
    "                \n",
    "                \n",
    "                #print(len(cat_df[col_name[i]].unique()))\n",
    "\n",
    "                #print(col_name[i])\n",
    "                #print(df[col_name[i]].unique())\n",
    "                #print(len(train[col_name[i]].unique()))\n",
    "                #copy the categorical columns\n",
    "                \n",
    "                \n",
    "                cat_df[col_name[i]]=df[col_name[i]]\n",
    "                first_value=cat_df[col_name[i]][0]\n",
    "                \n",
    "                \n",
    "                if(col_name[i]==y):\n",
    "                    from sklearn import utils\n",
    "                    f.write('from sklearn import preprocessing\\n')\n",
    "                \n",
    "                    from sklearn import preprocessing\n",
    "                    f.write('#Label Encoded  '+y)\n",
    "                    f.write(\"\\n\")\n",
    "                    le = preprocessing.LabelEncoder()\n",
    "                    f.write('le = preprocessing.LabelEncoder()\\n')\n",
    "\n",
    "                    cat_df[col_name[i]]=le.fit_transform(cat_df[col_name[i]])\n",
    "                    f.write('df[\"'+y+'\"]=le.fit_transform(df[\"'+y+'\"])\\n')\n",
    "                    #print(cat_df[col_name[i]].dtypes)\n",
    "                   \n",
    "                    \n",
    "\n",
    "\n",
    "                    #cat_df[col_name[i]]= cat_df[col_name[i]].apply(le.fit_transform)\n",
    "                    #print(cat_df[col_name[i]])\n",
    "                    df=df.drop([col_name[i]],axis=1)\n",
    "                    df=pd.concat([df,cat_df],axis=1)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                elif( len(str(first_value))>15 or cat_df[col_name[i]][0].count(' ')>4):\n",
    "                    \n",
    "                    \n",
    "                    #print(\"entered loop\")\n",
    "                    f.write(\"#\"+str(col_name[i])+\" identified to be a text\\n\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "                    \n",
    "                    cv = TfidfVectorizer(lowercase=True,stop_words='english',tokenizer = token.tokenize)\n",
    "                    \n",
    "                    text_counts= cv.fit_transform(cat_df[col_name[i]].values)\n",
    "                    \n",
    "                    dfOneHot =pd.DataFrame(columns=cv.get_feature_names(),data=text_counts.toarray())\n",
    "                    \n",
    "                    lines=inspect.getsource(regex_token)\n",
    "                    f.write(lines)\n",
    "                    f.write(\"\\n\")\n",
    "                    var=str(col_name[i])\n",
    "                    f.write('dfOneHot=regex_token(df,\"'+var+'\")\\n')\n",
    "                    f.write('df=df.drop([\"'+var+'\"],axis=1)\\n')\n",
    "                    f.write('df=pd.concat([df,dfOneHot],axis=1)\\n')\n",
    "\n",
    "                    cat_df = pd.concat([cat_df, dfOneHot], axis=1)\n",
    "                    #print(cat_df)\n",
    "\n",
    "\n",
    "                    cat_df=cat_df.drop([col_name[i]],axis=1)\n",
    "\n",
    "                    df=df.drop([col_name[i]],axis=1)\n",
    "                    new_df=pd.concat([new_df,cat_df],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "                elif ((len(cat_df[col_name[i]].unique())/len(cat_df))<0.21):\n",
    "                    \n",
    "                    \n",
    "                    f.write(\"\\n#One-Hot Encoding done for \"+str(col_name[i]))\n",
    "                    cat=pd.DataFrame()\n",
    "                    cat=cat_df[[col_name[i]]]\n",
    "\n",
    "                    \n",
    "                    f.write('\\n')\n",
    "                    \n",
    "                \n",
    "                    X1=one_hot(cat[col_name[i]],cat)\n",
    "                    \n",
    "                \n",
    "\n",
    "                    dfOneHot = pd.DataFrame(X1, columns = [str(col_name[i])+\"_\"+str((k)) for k in range(X1.shape[1])])\n",
    "                    cat_df = pd.concat([cat_df, dfOneHot], axis=1)\n",
    "                    var=str(col_name[i])\n",
    "                    f.write('af=pd.get_dummies(df[\"'+var+'\"],prefix=\"'+var+'\")\\n')\n",
    "                    f.write('df=df.drop([\"'+var+'\"],axis=1)\\n')\n",
    "                    f.write('df=pd.concat([df,af],axis=1)\\n')\n",
    "                    \n",
    "\n",
    "\n",
    "                    cat_df=cat_df.drop([col_name[i]],axis=1)\n",
    "\n",
    "                    df=df.drop([col_name[i]],axis=1)\n",
    "                    df=pd.concat([df,cat_df],axis=1)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    \n",
    "                    f.write(\"\\n#Frequency Encoding done for \"+str(col_name[i])+\" due to high cardinality categorical attribute values\")\n",
    "                    f.write('\\n')\n",
    "                    f.write('enc_nom = (df.groupby(\"'+col_name[i]+'\").size()) / len(df)\\n')\n",
    "                    \n",
    "                    enc_nom = (cat_df.groupby(col_name[i]).size()) / len(train)\n",
    "                    #print(enc_nom)\n",
    "\n",
    "                    \n",
    "                    cat_df[col_name[i]]=df[col_name[i]].map(enc_nom)\n",
    "                    f.write('\\ndf[\"'+col_name[i]+'\"]=df[\"'+col_name[i]+'\"].map(enc_nom)\\n')\n",
    "                    df=df.drop([col_name[i]],axis=1)\n",
    "                    df=pd.concat([df,cat_df],axis=1)\n",
    "            else:\n",
    "                f.write('\\n#Normalizing the data so that the data approximately follows a Gaussian distribution '+str(col_name[i]))\n",
    "                f.write('\\n')\n",
    "            \n",
    "                stand_val=df[col_name[i]].values\n",
    "                f.write('stand_val=df[\"'+col_name[i]+'\"].values\\n')\n",
    "                scaler = StandardScaler()\n",
    "                f.write(\"scaler=StandardScaler()\\n\")\n",
    "                df[col_name[i]]=scaler.fit_transform(stand_val.reshape(-1,1))\n",
    "                f.write('df[\"'+col_name[i]+'\"]=scaler.fit_transform(stand_val.reshape(-1,1))\\n')\n",
    "    \n",
    "                \n",
    "            \n",
    "                    \n",
    "            \n",
    "    except Exception as ex:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"Ooopsss !!!!\")\n",
    "        \n",
    "        sys.exit(ex)\n",
    "        \n",
    "        \n",
    "    #imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    #df=pd.DataFrame(imp.fit_transform(df),columns=train.columns,index=train.index)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "    \n",
    "    #print(df)\n",
    "    #print(df.dtypes)\n",
    "    df=pd.concat([df,new_df],axis=1)\n",
    "    f.write(\"\\n#Done with Categorical Encoding\")\n",
    "    f.write(\"\\n\")\n",
    "    print(\"Done with Categorical encoding !!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#finds the perctenge of null values for each variable\n",
    "#df_train is the dataset\n",
    "def null_val_perc(df_train,f):\n",
    "    total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df_train.isnull().sum()/df_train.isnull().count()*100).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    print(\"Do we have missing data ?\")\n",
    "    print(missing_data.head(20))\n",
    "    #drops features whose null value percentage is greater than 99 perc\n",
    "    to_drop=missing_data[missing_data['Percent']>99.0]\n",
    "    to_drop=to_drop.index.values.tolist()\n",
    "    for dro in to_drop:\n",
    "        f.write('df=df.drop([\"'+to_drop+'\",axis=1)\\n')\n",
    "\n",
    "    df_train=df_train.drop(to_drop,axis=1)\n",
    "    return df_train\n",
    "\n",
    "#performs EDA\n",
    "#output_df is the dataset\n",
    "def preprocess(output_df):\n",
    "    col_name=[]\n",
    "    col_name=output_df.columns\n",
    "    for i in range(len(col_name)):\n",
    "        print(output_df[col_name[i]][0])\n",
    "        #x=re.search(r'\\d{1,2}\\/\\d{1,2}\\/\\d{4}$',str(output_df[col_name[i]][0]))\n",
    "        #if x!=None:\n",
    "            \n",
    "            #if (x.group()==output_df[col_name[i]][0]):\n",
    "                #print(\"Reached\")\n",
    "                #output_df[col_name[i]]=pd.to_datetime(output_df[col_name[i]])\n",
    "                #print(output_df[col_name[i]].dt.year)\n",
    "                \n",
    "    \n",
    "        print(\"For the Column:\", col_name[i])\n",
    "        print(\"Data type is:\",output_df[col_name[i]].dtypes)\n",
    "        print(\"Number of unique values\",len(output_df[col_name[i]].unique()))\n",
    "        print(\"Value Counts\",output_df[col_name[i]].value_counts())\n",
    "        print(\"Missing Values:\",output_df[col_name[i]].isnull().value_counts())\n",
    "        print(\"About the data:\\n\",output_df[col_name[i]].describe())\n",
    "        #sns.set(rc={'figure.figsize':(20,10)})\n",
    "        \n",
    "        \n",
    "        \n",
    "        if np.issubdtype(output_df[col_name[i]],np.number):\n",
    "            #fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "            #sns.distplot(output_df[col_name[i]].dropna(),ax=ax1)\n",
    "            #sns.boxplot(output_df[col_name[i]].dropna(),ax=ax2)\n",
    "            #sns.set(rc={'figure.figsize':(20,10)})\n",
    "            #plt.show()\n",
    "            \n",
    "            f=output_df[col_name[i]]\n",
    "            f_Q1=f.quantile(0.25)\n",
    "            f_Q3=f.quantile(0.75)\n",
    "            f_IQR=f_Q3-f_Q1\n",
    "            f_lowerend=f_Q1-(1.5*f_IQR)\n",
    "            f_upperend=f_Q3+(1.5*f_IQR)\n",
    "            f_outliers=f[(f<f_lowerend) | (f>f_upperend)]\n",
    "            f_outliers\n",
    "            print(\"Outliers are\",f_outliers.value_counts().keys().tolist())\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "          \n",
    "        \"\"\"\n",
    "          \n",
    "            \n",
    "        elif np.issubdtype(output_df[col_name[i]],np.object):\n",
    "            \n",
    "            if (np.issubdtype(output_df[col_name[i]],np.datetime64)==False):\n",
    "                #plt.figure()\n",
    "                #sns.countplot(output_df[col_name[i]].dropna())\n",
    "                #sns.set(rc={'figure.figsize':(20,10)})\n",
    "                #plt.xticks(rotation=90)\n",
    "                #plt.show()\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        \"\"\" \n",
    "    return output_df\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#file is the location of the file\n",
    "#y is the target variable\n",
    "#op is the kind of problem\n",
    "def start(file,y,op):\n",
    "    \n",
    "    print('Reading the file....')\n",
    "    extension=re.split(r'\\.',file)\n",
    "    try:\n",
    "        \n",
    "    \n",
    "        if extension[1]=='excel':\n",
    "            train=pd.read_excel(file)\n",
    "        elif extension[1]=='csv':\n",
    "            train=pd.read_csv(file,encoding='utf-8')\n",
    "        elif extension[1]=='json':\n",
    "            train=pd.read_json(file)\n",
    "        elif extension[1]=='txt':\n",
    "            with open(file, 'r') as f:\n",
    "                first_line=f.readline()\n",
    "                #print(first_line)\n",
    "                r1=re.findall(r'[`\\-=~!@#$%^&*+\\[\\]{};\\'\\\\:\"|<,.<>?]',first_line)\n",
    "                train=pd.read_csv(file,sep=r1[0])\n",
    "                \n",
    "        elif extension[1]=='log':\n",
    "            \n",
    "            with open(file,encoding='utf-8') as f:\n",
    "                \n",
    "                lines = f.read().splitlines()\n",
    "                lines = [lines[x:x+3] for x in range(0, len(lines), 3)]\n",
    "\n",
    "\n",
    "                with open('new_file.csv', 'w+',encoding='utf-8') as csvfile:\n",
    "                    \n",
    "                    w = csv.writer(csvfile)\n",
    "                    w.writerows(lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            train=pd.read_csv(\"new_file.csv\",delimiter=\"\\t\",header=None)\n",
    "            if len(train.columns.values.tolist())==1:\n",
    "                print(\"It's a Non-Delimited log file\")\n",
    "                print(\"Cannot read the file\")\n",
    "                return None\n",
    "\n",
    "             #train=pd.read_csv(file,sep='\\t',header=None)\n",
    "            #with open(file,'r') as file:\n",
    "               \n",
    "            \n",
    "        else:\n",
    "            print(\"Sorry !!! \")\n",
    "            print(\"Cannot read the file\")\n",
    "            return None\n",
    "    except Exception as ex:\n",
    "        \n",
    "        print(\"OOPS!!\")\n",
    "        sys.exit(ex)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #train=file\n",
    "    print('File has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "    print(\"\\n Types of features\")\n",
    "    print(train.dtypes)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Column names\")\n",
    "    col = train.columns.values.tolist()\n",
    "    print(col)\n",
    "    train=preprocess(train)\n",
    "    f=open('DataScience_Lifecycle3.py','w+')\n",
    "    f.write('import pandas as pd\\n')\n",
    "    f.write('file=input(\"Enter the file location\")\\n')\n",
    "    f.write('df=pd.read_csv(file)\\n')\n",
    "    #train=train[:100]\n",
    "    for features in col:\n",
    "        if(((len(train[features].unique())==len(train))or(len(train[features].unique())==1))and(features!=y)):\n",
    "        \n",
    "            f.write('df=df.drop([\"'+features+'\"],axis=1)\\n')\n",
    "            train=train.drop([features],axis=1)\n",
    "    \n",
    "            \n",
    "    train= null_val_perc(train,f)\n",
    "    \n",
    "    for column in train.columns:\n",
    "        train[column].fillna(train[column].mode()[0], inplace=True)\n",
    "        f.write('df[\"'+column+'\"].fillna(train[\"'+column+'\"].mode()[0],inplace=True)\\n')\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    #print(train.head(5))\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #train=find_out(train)\n",
    "    \n",
    "    \n",
    "    #train=preprocess(train)\n",
    "   \n",
    "    def feature_engineering(train,f):\n",
    "        f.write('#Starting Feature engineering\\n')\n",
    "        train=find(train,f)\n",
    "        \n",
    "        train= null_val_perc(train,f)\n",
    "    \n",
    "    \n",
    "    \n",
    "        for column in train.columns:\n",
    "        \n",
    "            train[column].fillna(train[column].mode()[0], inplace=True)\n",
    "            f.write('df[\"'+column+'\"].fillna(train[\"'+column+'\"].mode()[0],inplace=True)\\n')\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "        train.to_csv('befor_cat_encoding.csv')\n",
    "        \n",
    "    #print(train.isna().sum())\n",
    "    \n",
    "    \n",
    "        train=cat_encoding(train,y,f)\n",
    "        train= null_val_perc(train,f)\n",
    "        \n",
    "        \n",
    "        train.to_csv('after _cat_encoding.csv')\n",
    "        return train\n",
    "        \n",
    "    copy_train=train\n",
    "    train=feature_engineering(train,f)\n",
    "    #print(train)\n",
    "    col_names=train.columns.values.tolist()\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    if y==None:\n",
    "        \n",
    "       \n",
    "       \n",
    "       \n",
    "  \n",
    "    # Normalizing the data so that  \n",
    "    # the data approximately follows a Gaussian distribution \n",
    "       \n",
    "        #f.write(\"\\n#Normalizing the data so that the data approximately follows a Gaussian distribution \")\n",
    "        \n",
    "  \n",
    "    # Converting the numpy array into a pandas DataFrame \n",
    "        #train = pd.DataFrame(,columns=col_names)\n",
    "        \n",
    "        #print(train)\n",
    "        \n",
    "        Clustering(train,f,copy_train)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif (y!=None) and (y not in col):\n",
    "        \n",
    "        print(\"Enter a valid Independent Variable !\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    elif y in col:\n",
    "        \n",
    "        #col.remove(y)\n",
    "        call(train,col_names,op,y,f)\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print('Relationship between {} and other features'.format(y))\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    for a in col:\n",
    "        \n",
    "        \n",
    "\n",
    "        print(' {} VS {} '.format(a, y))\n",
    "        fea=train.groupby(a)\n",
    "        fea[y].sum().plot()\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        print('Distribution of {} by {}'.format(a,y))\n",
    "        lp=[]\n",
    "        for i in train[a].unique():\n",
    "            lp.append(len(train.ix[train[a]==i,y])/len(train))\n",
    "\n",
    "    \n",
    "        city_rev=pd.DataFrame(columns=['Average {}'.format(y)],index=train[a].unique(),data=lp)\n",
    "        pk=[]\n",
    "        for i in train[a].unique():\n",
    "            \n",
    "            pk.append(len(train.ix[train[a]==i,y]))\n",
    "            \n",
    "            \n",
    "        print(city_rev.head(7))\n",
    "            \n",
    "\"\"\"            \n",
    "        \n",
    "        \n",
    "#train is the dataset\n",
    "#col is the list of column names\n",
    "#op is the operation\n",
    "#y is the target variable\n",
    "#f is the file pointer\n",
    "def call(train,col,op,y,f):\n",
    "\n",
    "   \n",
    "    \n",
    "    if op=='Regression':\n",
    "    \n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "        Regression(train,y,f)\n",
    "        \n",
    "        \n",
    "    elif op=='Classification':\n",
    "        \n",
    "        #lines=inspect.getsource(call)\n",
    "        #f.write(lines)\n",
    "        #f.write(\"\\n\")\n",
    "        \n",
    "        Classification(train,y,f)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #elif op=='Clustering':\n",
    "        #Clustering(train)\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "        \n",
    "def regression_tuning(X_train, x_test, y_train, y_test,f):\n",
    "    try:\n",
    "        \n",
    "    \n",
    "        n_estimators = [10,20,30,40,50,60,70,80,90,100,120,150,200,300]\n",
    "        max_features = ['auto', 'sqrt','log2']\n",
    "        max_leaf_nodes=[1,2,3,4,5,6,7,8,9,10]\n",
    "        max_leaf_nodes.append(None)\n",
    "        max_depth = [1,2,3,4,5,6,7,8,9,10]\n",
    "        max_depth.append(None)\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        min_samples_leaf = [1, 2,3, 4,5]\n",
    "        bootstrap = [True, False]\n",
    "        models = {\n",
    "        \"RandomForestRegressor\":RandomForestRegressor(),\n",
    "        \"ElasticNet\":ElasticNet(),\n",
    "         \"XGBRegressor\":XGBRegressor(),\n",
    "       \"GradientBoostingRegressor\":GradientBoostingRegressor(),\n",
    "        \"ExtraTreesRegressor\":ExtraTreesRegressor(),\n",
    "        \"LinearRegression\":LinearRegression(),\n",
    "        \"AdaBoostRegressor\":AdaBoostRegressor(),\n",
    "       \"LGBMRegressor\":LGBMRegressor(),\n",
    "        \"Lasso\":Lasso(),\n",
    "        \"Ridge\":Ridge(),\n",
    "        \"SVR\":SVR(),\n",
    "        \"DecisionTreeRegressor\":DecisionTreeRegressor()\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        params={\n",
    "        \"RandomForestRegressor\":{\n",
    "\n",
    "            'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "            'max_leaf_nodes':max_leaf_nodes,\n",
    "\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'bootstrap':bootstrap\n",
    "\n",
    "\n",
    "            },\n",
    "            \"DecisionTreeRegressor\":{\n",
    "                'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "            'max_leaf_nodes':max_leaf_nodes,\n",
    "\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf\n",
    "\n",
    "            },\n",
    "\n",
    "            \"ElasticNet\":{\"max_iter\": [1, 5, 10],\n",
    "                          \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                          \"l1_ratio\": np.arange(0.0, 1.0, 0.1)\n",
    "                },\n",
    "\n",
    "            \"GradientBoostingRegressor\":{\n",
    "                'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "                'learning_rate':[0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf\n",
    "\n",
    "\n",
    "\n",
    "            },\n",
    "            \"ExtraTreesRegressor\":{\n",
    "                 'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'bootstrap':bootstrap,\n",
    "            'n_estimators':n_estimators\n",
    "\n",
    "            },\n",
    "            \"LinearRegression\":{\n",
    "                \"fit_intercept\":[True]\n",
    "            },\n",
    "\n",
    "            \"AdaBoostRegressor\":\n",
    "            {'n_estimators': [10,20,30,40,50,60,70,80,90,100,120,150,200],\n",
    "         'learning_rate' : [0.01,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "         'loss' : ['linear', 'square', 'exponential']\n",
    "\n",
    "\n",
    "\n",
    "            },\n",
    "\n",
    "            \"SVR\":\n",
    "            {\n",
    "                'kernel':['rbf','linear','poly'],\n",
    "                'C':[1.0]\n",
    "            },\n",
    "\n",
    "            \"LGBMRegressor\":{\n",
    "                'boosting_type':['gbdt','rf'],\n",
    "                'objective':['regression'],\n",
    "                'metric':['rmse'],\n",
    "                'max_depth':[1,2,3,4,5,6,7,8,9,10],\n",
    "                'num_leaves':[30,40,50,60,70,80,90],\n",
    "                'learning_rate':[0.01,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1 ],\n",
    "                'feature_fraction':[0.9,1.0],\n",
    "                'bagging_fraction':[0.9],\n",
    "                'bagging_freq':[70]\n",
    "            },\n",
    "            \"XGBRegressor\":{\n",
    "                'booster':['gbtree','gblinear','dart'],\n",
    "                'learning_rate':[0.01,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                'max_depth':[1,2,3,4,5,6,7,8,9,10]\n",
    "            },\n",
    "\n",
    "            \"Lasso\":{\n",
    "                'alpha':[0.01,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "\n",
    "            },\n",
    "\n",
    "\n",
    "            \"Ridge\":{\n",
    "                'alpha':[0.01,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                'solver':['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "\n",
    "\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "        #Creating a dataframe to store parameter values for different models\n",
    "        Parameters=pd.DataFrame(columns=['kernel','C','solver','max_leaf_nodes','booster','max_features', 'max_depth','min_samples_split','min_samples_leaf','bootstrap','max_iter',\n",
    "                                'alpha','l1_ratio','fit_intercept','n_estimators','loss','boosting_type','bagging_fraction','bagging_freq','objective','metric','num_leaves','learning_rate','feature_fraction','rmse','max_error','r2_score','explained_variance_score','mae','mse','Time'],\n",
    "                             index=[\"XGBRegressor\",\"RandomForestRegressor\", \"ElasticNet\",\"GradientBoostingRegressor\",\"ExtraTreesRegressor\",\n",
    "                                    \"LinearRegression\",\"AdaBoostRegressor\",\"LGBMRegressor\",\"Lasso\",\"Ridge\",\"SVR\",\"DecisionTreeRegressor\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                search_val=input(\"Want to go for GridSearchCV(g) or RandomizedSearchCV(r) :\\n\")\n",
    "                if((search_val=='g')or(search_val=='r')):\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Enter a valid string!\\n\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        for name in models.keys():\n",
    "\n",
    "            est = models[name]\n",
    "            est_params = params[name]\n",
    "            then=time.time()\n",
    "            if search_val=='g':\n",
    "                gscv = GridSearchCV(estimator=est, param_grid=est_params,cv=5)\n",
    "            elif search_val=='r':\n",
    "                gscv = RandomizedSearchCV(estimator=est, param_distributions=est_params,cv=5,n_iter=100)\n",
    "\n",
    "\n",
    "\n",
    "            import warnings\n",
    "            warnings.filterwarnings('ignore')\n",
    "            gscv.fit(X_train,y_train)\n",
    "            now=time.time()\n",
    "            Time=now-then\n",
    "            print(\"Best parameters are: {}\".format(gscv.best_estimator_))\n",
    "            rr=gscv.cv_results_['params'][gscv.best_index_]\n",
    "\n",
    "\n",
    "            rms = np.sqrt(metrics.mean_squared_error(y_test, (gscv.predict(x_test))))\n",
    "            print(\"RMSE is\",rms)\n",
    "            R_square=r2_score(y_test,(gscv.predict(x_test)))\n",
    "            mae=mean_absolute_error(y_test,(gscv.predict(x_test)))\n",
    "            mse=mean_squared_error(y_test,(gscv.predict(x_test)))\n",
    "            evs=explained_variance_score(y_test,(gscv.predict(x_test)))\n",
    "            me=max_error(y_test,(gscv.predict(x_test)))\n",
    "            print(\"R_square is \",R_square)\n",
    "            print(\"Mean_absolute_error\",mae)\n",
    "            print(\"Mean_squared_error\",mse)\n",
    "            print(\"Max_error\",me)\n",
    "            print(\"Explained_variance_score\",evs)\n",
    "\n",
    "            for x in rr:\n",
    "                Parameters.loc[name][x]=rr[x]\n",
    "\n",
    "            Parameters.loc[name]['rmse']=rms\n",
    "            Parameters.loc[name]['Time']=Time\n",
    "            Parameters.loc[name]['r2_score']=R_square\n",
    "            Parameters.loc[name]['mae']=mae\n",
    "            Parameters.loc[name]['mse']=mse\n",
    "            Parameters.loc[name]['explained_variance_score']=evs\n",
    "            Parameters.loc[name]['max_error']=me\n",
    "        Parameters.to_csv(\"Parameter_reglist.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        min_rms=Parameters['rmse'].min()\n",
    "        para=Parameters.loc[Parameters['rmse']==min_rms]\n",
    "\n",
    "        model_name=para.index.values\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Final Model is \",model_name[0])\n",
    "\n",
    "        if(model_name[0]=='RandomForestRegressor'):\n",
    "\n",
    "            mm=RandomForestRegressor(max_features=para['max_features'].item(),max_depth=para['max_depth'].item(),min_samples_split=para['min_samples_split'].item(),min_samples_leaf=para['min_samples_leaf'].item(),bootstrap=para['bootstrap'].item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='ElasticNet'):\n",
    "            mm=ElasticNet(max_iter=para['max_iter'].item(),alpha=para['alpha'].item(),l1_ratio=para['l1_ratio'].item())\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='DecisionTreeRegressor'):\n",
    "            mm=DecisionTreeRegressor(max_features=para['max_features'].item(),max_depth=para['max_depth'].item(),min_samples_split=para['min_samples_split'].item(),min_samples_leaf=para['min_samples_leaf'].item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='GradientBoostingRegressor'):\n",
    "            mm=GradientBoostingRegressor(max_features=para['max_features'].item(),max_depth=para['max_depth'].item(),min_samples_split=para['min_samples_split'].item(),min_samples_leaf=para['min_samples_leaf'].item())\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='ExtraTreesRegressor'):\n",
    "            mm=ExtraTreesRegressor(max_features=para['max_features'].item(),min_samples_split=para['min_samples_split'].item(),min_samples_leaf=para['min_samples_leaf'].item(),bootstrap=para['bootstrap'].item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='LinearRegression'):\n",
    "            mm=LinearRegression(fit_intercept=para['fit_intercept'].item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='Lasso'):\n",
    "            mm=Lasso(alpha=para['alpha'].item())\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='Ridge'):\n",
    "            mm=Ridge(alpha=para['alpha'].item(),solver=para['solver'].item())\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='AdaBoostRegressor'):\n",
    "            mm=AdaBoostRegressor(n_estimators=para['n_estimators'].item(),loss=para['loss'].item())\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='SVR'):\n",
    "            mm=SVR(kernel=para['kernel'].item(),C=para['C'].item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='LGBMRegressor'):\n",
    "            mm=LGBMRegressor(feature_fraction=para['feature_fraction'].item(),bagging_fraction=para['bagging_fraction'].item(),num_leaves=para['num_leaves'].item(),max_depth=para['max_depth'].item(),bagging_freq=para['bagging_freq'].item(),boosting_type=para['boosting_type'].item(),objective=para['objective'].item(),metric=para['metric'].item())\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='XGBRegressor'):\n",
    "            mm=XGBRegressor(booster=para['booster'].item(),learning_rate=para['learning_rate'].item(),max_depth=para['max_depth'].item())\n",
    "\n",
    "        mm.fit(X_train,y_train)\n",
    "        rms = np.sqrt(metrics.mean_squared_error(y_test, (mm.predict(x_test))))\n",
    "        print(\"RMSE is\",para['rmse'].item())\n",
    "        filename = 'finalized_model.sav'\n",
    "        pickle.dump(mm, open(filename, 'wb'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#Final Model is \"+model_name[0]+\"\\n\")\n",
    "        f.write('model=pickle.load(\"'+filename+'\")\\n')\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print(\"Someting went wrong !!!!\")\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "#df is the dataset\n",
    "#y is the target variable\n",
    "#f is the file pointer\n",
    "def Regression(df,y,f):\n",
    "    print(\"Getting ready for regression\")\n",
    "    X=df[df.columns.difference([y])]\n",
    "    Y=df[y]\n",
    "    n=get_pca(X)\n",
    "    #print(n)\n",
    "    pca = PCA(n_components=n)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    principalDf = pd.DataFrame(data = principalComponents)\n",
    "    principalDf.to_csv('principalDf.csv')\n",
    "    f.write(\"\\n#Done PCA, total number of features \"+str(n))\n",
    "    \n",
    "    seed = 29\n",
    "    X_train, x_test, y_train, y_test = train_test_split(principalDf, Y, test_size=0.2, random_state=seed)\n",
    "    regression_tuning(X_train, x_test, y_train, y_test,f)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def classification_tuning(X_train, x_test, y_train, y_test,f):\n",
    "    try:\n",
    "        \n",
    "    \n",
    "        n_estimators = [10,20,30,40,50,60,70,80,90,100,120,150,200]\n",
    "    # Number of features to consider at every split\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [1,2,3,4,5,6,7,8,9,10]\n",
    "        max_depth.append(None)\n",
    "        max_leaf_nodes=[1,2,3,4,5,6,7,8,9,10]\n",
    "        max_leaf_nodes.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        models = {\n",
    "        \"RandomForestClassifier\":RandomForestClassifier(),\n",
    "        \"KNeighborsClassifier\":KNeighborsClassifier(),\n",
    "        \"GradientBoostingClassifier\":GradientBoostingClassifier(),\n",
    "        \"ExtraTreesClassifier\":ExtraTreesClassifier(),\n",
    "        \"LogisticRegression\":LogisticRegression(),\n",
    "        \"SVC\":SVC(),\n",
    "\n",
    "        \"MLPClassifier\":MLPClassifier(),\n",
    "            \"XGBClassifier\":XGBClassifier(),\n",
    "            \"AdaBoostClassifier\":AdaBoostClassifier(),\n",
    "            \"LGBMClassifier\":LGBMClassifier(),\n",
    "            \"DecisionTreeClassifier\":DecisionTreeClassifier()\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        params={\n",
    "\n",
    "        \"RandomForestClassifier\":{\n",
    "\n",
    "            'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'bootstrap':bootstrap\n",
    "\n",
    "\n",
    "            },\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \"KNeighborsClassifier\":{'n_neighbors':[5,6,7,8,9,10],\n",
    "              'leaf_size':[1,2,3,5],\n",
    "              'weights':['uniform', 'distance'],\n",
    "              'algorithm':['auto', 'ball_tree','kd_tree','brute']\n",
    "                },\n",
    "\n",
    "            \"GradientBoostingClassifier\":{\n",
    "                'loss':['deviance'],\n",
    "                'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate' : [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf\n",
    "\n",
    "            },\n",
    "            \"ExtraTreesClassifier\":{\n",
    "                 'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'bootstrap':bootstrap\n",
    "\n",
    "            },\n",
    "\n",
    "            \"SVC\":{\n",
    "                'kernel':['rbf','linear','poly']\n",
    "            },\n",
    "            \"LogisticRegression\":{\n",
    "                'penalty':['l2'],\n",
    "                'C':[1.0]\n",
    "\n",
    "            },\n",
    "\n",
    "            \"DecisionTreeClassifier\":\n",
    "            {\n",
    "              'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "            'max_leaf_nodes':max_leaf_nodes,\n",
    "\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf  \n",
    "            },\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \"MLPClassifier\":{'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,150,100)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive']\n",
    "                    },\n",
    "\n",
    "            \"XGBClassifier\":{'min_child_weight': [1, 5, 10],\n",
    "            \"learning_rate\" : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n",
    "            'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'objective':['reg:squarederror','count:poisson']\n",
    "\n",
    "            },\n",
    "\n",
    "\n",
    "            \"AdaBoostClassifier\":{'n_estimators':n_estimators,\n",
    "            \"learning_rate\" : [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "            },\n",
    "            \"LGBMClassifier\":{'boosting_type':['gbdt','rf'],\n",
    "                'objective':['multiclass','binary'],\n",
    "                'metric':['binary_error','multi_logloss'],\n",
    "                'max_depth':[1,2,3,4,5,6,7,8,9,10,None],\n",
    "                'num_leaves':[30,40,50,60,70,80,90],\n",
    "                'learning_rate':[0.05, 0.10, 0.15, 0.20, 0.25, 0.30,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "                'feature_fraction':[0.9,1.0],\n",
    "                'bagging_fraction':[0.9],\n",
    "                'bagging_freq':[70]\n",
    "\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "             }\n",
    "\n",
    "\n",
    "        Parameters=pd.DataFrame(columns=['objective','penalty','C','kernel','boosting_type','objective','metric','max_depth','num_leaves','learning_rate','feature_fraction',\n",
    "                               'bagging_fraction','bagging_freq','n_estimators','colsample_bytree','subsample','gamma','min_child_weight',\n",
    "                               'alpha','solver','activation','hidden_layer_sizes','min_samples_split','min_samples_leaf',\n",
    "                               'bootstrap','max_features','loss','max_depth','weights','algorithm','leaf_size','n_neighbors','accuracy','Time','auc','average_precision_score','precision_score','log_loss','f1_score'],\n",
    "                               index=[\"RandomForestClassifier\",\"KNeighborsClassifier\",\"GradientBoostingClassifier\",\"DecisionTreeClassifier\",\n",
    "       \"ExtraTreesClassifier\",\"MLPClassifier\",\"XGBClassifier\",\"AdaBoostClassifier\",\"LGBMClassifier\",\"LogisticRegression\",\"SVC\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                search_val=input(\"Want to go for GridSearchCV(g) or RandomizedSearchCV(r) :\\n\")\n",
    "                if((search_val=='g')or(search_val=='r')):\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Enter a valid string!\\n\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        for name in models.keys():\n",
    "\n",
    "            est = models[name]\n",
    "            est_params = params[name]\n",
    "            then=time.time()\n",
    "            if search_val=='g':\n",
    "                gscv = GridSearchCV(estimator=est, param_grid=est_params,cv=5)\n",
    "            elif search_val=='r':\n",
    "                gscv = RandomizedSearchCV(estimator=est, param_distributions=est_params,cv=5,n_iter=100)\n",
    "\n",
    "\n",
    "            import warnings\n",
    "            warnings.filterwarnings('ignore')\n",
    "            gscv.fit(X_train,y_train)\n",
    "            #print(y_train.shape)\n",
    "            now=time.time()\n",
    "            Time=now-then\n",
    "            print(\"\\n\")\n",
    "            print(\"Best parameters are: {}\".format(gscv.best_estimator_))\n",
    "            rr=gscv.cv_results_['params'][gscv.best_index_]\n",
    "\n",
    "\n",
    "            #gscv.fit(X_train,y_train)\n",
    "            acc = metrics.accuracy_score(gscv.predict(x_test),y_test)\n",
    "            print(\"Accuracy is \",acc)\n",
    "            f1_score1=f1_score(y_test,gscv.predict(x_test),average='weighted')\n",
    "            print(\"F1 Score\",f1_score1)\n",
    "            if(len(y_test.unique())<3):\n",
    "                \n",
    "            \n",
    "                auc1=roc_auc_score(y_test,gscv.predict(x_test),average='weighted',multi_class='ovo')\n",
    "                print(\"Area under the curve\",auc1)\n",
    "            \n",
    "                apc=average_precision_score(y_test,gscv.predict(x_test))\n",
    "                print(\"Average precision Score\",apc)\n",
    "                ps=precision_score(y_test,gscv.predict(x_test))\n",
    "                print(\"Precision Score\",ps)\n",
    "                log_loss1=log_loss(y_test,gscv.predict(x_test))\n",
    "                print(\"Log Loss\",log_loss1)\n",
    "\n",
    "\n",
    "            for x in rr:\n",
    "                Parameters.loc[name][x]=rr[x]\n",
    "\n",
    "            Parameters.loc[name]['accuracy']=acc\n",
    "            Parameters.loc[name]['Time']=Time\n",
    "            \n",
    "            Parameters.loc[name]['f1_score']=f1_score1\n",
    "            if(len(y_test.unique())<3):\n",
    "                Parameters.loc[name]['auc']=auc1\n",
    "                \n",
    "                Parameters.loc[name]['average_precision_score']=apc\n",
    "                Parameters.loc[name]['precision_score']=ps\n",
    "                Parameters.loc[name]['log_loss']=log_loss1\n",
    "        Parameters.to_csv(\"Parameter_list1.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        max_acc=Parameters['accuracy'].max()\n",
    "        para=Parameters.loc[Parameters['accuracy']==max_acc]\n",
    "\n",
    "        model_name=para.index.values\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Final Model is \",model_name[0])\n",
    "\n",
    "        if(model_name[0]=='RandomForestClassifier'):\n",
    "            max_depth=para['max_depth'].values.tolist()\n",
    "    \n",
    "            num_max_depth=[val[0] for val in max_depth]\n",
    "            mm=RandomForestClassifier(max_depth=num_max_depth[0],max_features=para['max_features'].item(),min_samples_split=para['min_samples_split'].item(),min_samples_leaf=para['min_samples_leaf'].item(),bootstrap=para['bootstrap'].item())\n",
    "            \n",
    "        elif(model_name[0]=='KNeighborsClassifier'):\n",
    "            mm=KNeighborsClassifier(n_neighbors=para['n_neighbors'].item(),leaf_size=para['leaf_size'].item(),weights=para['weights'].item(),algorithm=para['algorithm'].item())\n",
    "\n",
    "        elif(model_name[0]==\"GradientBoostingClassifier\"):\n",
    "            max_depth=para['max_depth'].values.tolist()\n",
    "    \n",
    "            num_max_depth=[val[0] for val in max_depth]\n",
    "            mm=GradientBoostingClassifier(max_depth=num_max_depth[0],loss=para['loss'].item(),max_features=para['max_features'].item(),learning_rate=para['learning_rate'].item(),min_samples_split=para['min_samples_split'].item(),min_samples_leaf=para['min_samples_leaf'].item())\n",
    "            \n",
    "\n",
    "\n",
    "        elif(model_name[0]==\"ExtraTreesClassifier\"):\n",
    "            max_depth=para['max_depth'].values.tolist()\n",
    "    \n",
    "            num_max_depth=[val[0] for val in max_depth]\n",
    "    \n",
    "            mm=ExtraTreesClassifier(max_depth=num_max_depth[0],bootstrap=para['bootstrap'].item(),max_features=para['max_features'].item(),min_samples_split=para['min_samples_split'].item(),min_samples_leaf=para['min_samples_leaf'].item())\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='MLPClassifier'):\n",
    "            mm=MLPClassifier(hidden_layer_sizes=para['hidden_layer_sizes'].item(),activation=para['activation'].item(),solver=para['solver'].item(),alpha=para['alpha'].item(),learning_rate=para['learning_rate'].item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='XGBClassifier'):\n",
    "            mm=XGBClassifier(objective=para['objective'],min_child_weight=para['min_child_weight'].item(),learning_rate=para['learning_rate'].item(),gamma=para['gamma'].item(),subsample=para['subsample'].item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]==\"AdaBoostClassifier\"):\n",
    "            mm=AdaBoostClassifier(n_estimators=para['n_estimators'].item(),learning_rate=para['learning_rate'].item())\n",
    "\n",
    "        elif(model_name[0]==\"DecisionTreeClassifier\"):\n",
    "            max_depth=para['max_depth'].values.tolist()\n",
    "    \n",
    "            num_max_depth=[val[0] for val in max_depth]\n",
    "            mm=DecisionTreeClassifier(max_depth=num_max_depth[0],max_features=para['max_features'].item(),min_samples_split=para['min_samples_split'].item(),min_samples_leaf=para['min_samples_leaf'].item())\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif(model_name[0]=='LGBMClassifier'):\n",
    "            \n",
    "            max_depth=para['max_depth'].values.tolist()\n",
    "    \n",
    "            num_max_depth=[val[0] for val in max_depth]\n",
    "            \n",
    "            objective=para['objective'].values.tolist()\n",
    "            name_objective=[val[0] for val in objective]\n",
    "        \n",
    "            \n",
    "            mm=LGBMClassifier(max_depth=num_max_depth[0],feature_fraction=para['feature_fraction'].item(),bagging_fraction=para['bagging_fraction'].item(),num_leaves=para['num_leaves'].item(),bagging_freq=para['bagging_freq'].item(),boosting_type=para['boosting_type'].item(),objective=name_objective[0],metric=para['metric'].item())\n",
    "            \n",
    "\n",
    "        elif(model_name[0]=='LogisticRegression'):\n",
    "            mm=LogisticRegression(penalty=para['penalty'].item(),C=para['C'].item())\n",
    "           \n",
    "\n",
    "\n",
    "        elif(model_name[0]=='SVC'):\n",
    "            mm=SVC(kernel=para['kernal'].item())\n",
    "            \n",
    "\n",
    "        \n",
    "        mm.fit(X_train,y_train)\n",
    "        acc = metrics.accuracy_score(mm.predict(x_test).round(),y_test)\n",
    "        print(\"Accuracy is \",para['accuracy'].item())\n",
    "        print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(mm.predict(x_test).round(),y_test))\n",
    "        filename = 'finalized_model.sav'\n",
    "        pickle.dump(mm, open(filename, 'wb'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#Final Model is \"+model_name[0]+\"\\n\")\n",
    "        f.write('model=pickle.load(\"'+filename+'\")\\n')\n",
    "        f.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong !!\")\n",
    "        print(e)\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "#f is the file pointer    \n",
    "def Classification(df,y,f):\n",
    "    \n",
    "    X=df[df.columns.difference([y])]\n",
    "    col= X.columns.values.tolist()\n",
    "    Y=df[y]\n",
    "\n",
    "    n=get_pca(X)\n",
    "    #print(n)\n",
    "    pca = PCA(n_components=n)\n",
    "    f.write(\"\\n#Done PCA, total number of features \"+str(n))\n",
    "    f.write('\\npca=PCA(n_components='+str(n)+')\\n')\n",
    "    f.write('\\nprincipalComponents = pca.fit_transform(df)\\n')\n",
    "    f.write('principalDf = pd.DataFrame(data = principalComponents)\\n')\n",
    "    \n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    principalDf = pd.DataFrame(data = principalComponents)\n",
    "    principalDf.to_csv('principalDf.csv')\n",
    "    #print(principalDf)\n",
    "    seed = 29\n",
    "    X_train, x_test, y_train, y_test = train_test_split(principalDf, Y, test_size=0.2, random_state=seed)\n",
    "    print(\"Get Started with classification\")\n",
    "    classification_tuning(X_train, x_test, y_train, y_test,f)\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# to find the best k number of clusters for gaussian clustering\n",
    "def gaus_compare_k_(k_list, X,est):\n",
    "    \n",
    "    #X = X.select_dtypes(['number']).dropna()\n",
    "    # Run clustering with different k and check the metrics\n",
    "    silhouette_list = []\n",
    "\n",
    "    for p in k_list:\n",
    "\n",
    "        clusterer = est(n_components=p,covariance_type='full')\n",
    "        \n",
    "\n",
    "        clusterer.fit(X)\n",
    "        cluster_labels=clusterer.predict(X)\n",
    "        # The higher (up to 1) the better\n",
    "        s = round(metrics.silhouette_score(X, cluster_labels), 4)\n",
    "\n",
    "\n",
    "        silhouette_list.append(s)\n",
    "\n",
    "    # The higher (up to 1) the better\n",
    "    #print(silhouette_list)\n",
    "    key = silhouette_list.index(max(silhouette_list))\n",
    "\n",
    "    k = k_list.__getitem__(key)\n",
    "\n",
    "    #print(\"Best silhouette =\", max(silhouette_list), \" for k=\", k)\n",
    "\n",
    "    return k\n",
    "    \n",
    "def Clustering(X,f,copy_train):\n",
    "    n=get_pca(X)\n",
    "    #print(n)\n",
    "    pca = PCA(n_components=n)\n",
    "    f.write(\"\\n#Done PCA, total number of features \"+str(n))\n",
    "    f.write('\\n')\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    df = pd.DataFrame(data = principalComponents)\n",
    "    #print(df)\n",
    "    silh_score=pd.DataFrame(columns=['silhouette_score','K'],index=['GaussianMixture','KMeans','AgglomerativeClustering','DBSCAN','SpectralClustering'])\n",
    "        \n",
    "    \n",
    "    \n",
    "    models={\n",
    "        \"KMeans\":KMeans()\n",
    "        #\"AgglomerativeClustering\":AgglomerativeClustering(),\n",
    "       #\"DBSCAN\":DBSCAN(),\n",
    "       # \"SpectralClustering\":SpectralClustering(),\n",
    "        #\"GaussianMixture\":GaussianMixture()\n",
    "        \n",
    "        \n",
    "    }\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    for name in models.keys():\n",
    "        \n",
    "        \n",
    "        \n",
    "        if name=='DBSCAN':\n",
    "            db = DBSCAN(eps=0.5, min_samples=10) \n",
    "            db.fit(df)\n",
    "            print(db.labels_)\n",
    "            num_clust=metrics.silhouette_score(df, db.labels_)\n",
    "            silh_score.loc[name]['silhouette_score']=num_clust\n",
    "            labels = db.labels_\n",
    "            k= len(set(db.labels_)) - (1 if -1 in labels else 0)\n",
    "            \n",
    "            silh_score.loc[name]['K']=k\n",
    "            \n",
    "            \n",
    "        elif name=='KMeans':\n",
    "           \n",
    "            \n",
    "            def compare_k_(k_list, X):\n",
    "                # to find the best k number of clusters\n",
    "                #X = X.select_dtypes(['number']).dropna()\n",
    "                # Run clustering with different k and check the metrics\n",
    "                silhouette_list = []\n",
    "\n",
    "                for p in k_list:\n",
    "                    \n",
    "\n",
    "                    clusterer = KMeans(n_clusters=p,init = 'k-means++', random_state = 42)\n",
    "                    \n",
    "\n",
    "                    clusterer.fit_predict(X)\n",
    "                    \n",
    "                    # The higher (up to 1) the better\n",
    "                    s = round(metrics.silhouette_score(X, clusterer.labels_), 7)\n",
    "                    \n",
    "\n",
    "                    silhouette_list.append(s)\n",
    "\n",
    "                # The higher (up to 1) the better\n",
    "                #print(silhouette_list)\n",
    "                key = silhouette_list.index(max(silhouette_list))\n",
    "\n",
    "                k = k_list.__getitem__(key)\n",
    "\n",
    "                #print(\"Best silhouette =\", max(silhouette_list), \" for k=\", k)\n",
    "\n",
    "                return max(silhouette_list),k\n",
    "            \n",
    "            \n",
    "            num_clust,k=compare_k_([2,3,4,5,6,7,8,9,10,11,12,13,14,15],df)\n",
    "            silh_score.loc[name]['silhouette_score']=num_clust\n",
    "            silh_score.loc[name]['K']=k\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        elif name=='AgglomerativeClustering':\n",
    "            \n",
    "            def compare_A_(k_list, X):\n",
    "                # to find the best k number of clusters\n",
    "                #X = X.select_dtypes(['number']).dropna()\n",
    "                # Run clustering with different k and check the metrics\n",
    "                silhouette_list = []\n",
    "\n",
    "                for p in k_list:\n",
    "                    \n",
    "\n",
    "                    clusterer =AgglomerativeClustering(n_clusters=p)\n",
    "\n",
    "                    clusterer.fit_predict(X)\n",
    "                    # The higher (up to 1) the better\n",
    "                    s = round(metrics.silhouette_score(X, clusterer.labels_), 7)\n",
    "\n",
    "                    silhouette_list.append(s)\n",
    "\n",
    "                # The higher (up to 1) the better\n",
    "                #print(silhouette_list)\n",
    "                key = silhouette_list.index(max(silhouette_list))\n",
    "\n",
    "                k = k_list.__getitem__(key)\n",
    "\n",
    "                #print(\"Best silhouette =\", max(silhouette_list), \" for k=\", k)\n",
    "\n",
    "                return max(silhouette_list),k\n",
    "            \n",
    "            \n",
    "            num_clust,k=compare_A_([2,3,4,5,6,7,8,9,10,11,12,13,14,15],df)\n",
    "            silh_score.loc[name]['silhouette_score']=num_clust\n",
    "            silh_score.loc[name]['K']=k\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "        elif name=='GaussianMixture':\n",
    "            \n",
    "            def compare_G_(k_list, X):\n",
    "                # to find the best k number of clusters\n",
    "                #X = X.select_dtypes(['number']).dropna()\n",
    "                # Run clustering with different k and check the metrics\n",
    "                silhouette_list = []\n",
    "\n",
    "                for p in k_list:\n",
    "                    \n",
    "\n",
    "                    clusterer = GaussianMixture(n_components=p,covariance_type='full')\n",
    "\n",
    "        \n",
    "\n",
    "                    clusterer.fit(X)\n",
    "                    cluster_labels=clusterer.predict(X)\n",
    "        # The higher (up to 1) the better\n",
    "                    s = round(metrics.silhouette_score(X, cluster_labels), 4)\n",
    "\n",
    "                    silhouette_list.append(s)\n",
    "\n",
    "                # The higher (up to 1) the better\n",
    "                #print(silhouette_list)\n",
    "                key = silhouette_list.index(max(silhouette_list))\n",
    "\n",
    "                k = k_list.__getitem__(key)\n",
    "\n",
    "                #print(\"Best silhouette =\", max(silhouette_list), \" for k=\", k)\n",
    "\n",
    "                return max(silhouette_list),k\n",
    "            \n",
    "            num_clust,k=compare_G_([2,3,4,5,6,7,8,9,10,11,12,13,14,15],df)\n",
    "            silh_score.loc[name]['silhouette_score']=num_clust\n",
    "            silh_score.loc[name]['K']=k\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        elif name=='SpectralClustering':\n",
    "            \n",
    "            \n",
    "            \n",
    "            def compare_S_(k_list, X):\n",
    "                \n",
    "                # to find the best k number of clusters\n",
    "                #X = X.select_dtypes(['number']).dropna()\n",
    "                # Run clustering with different k and check the metrics\n",
    "                silhouette_list = []\n",
    "\n",
    "                for p in k_list:\n",
    "                    \n",
    "\n",
    "                    clusterer =SpectralClustering(n_clusters=p)\n",
    "\n",
    "                    clusterer.fit_predict(X)\n",
    "                    # The higher (up to 1) the better\n",
    "                    s = round(metrics.silhouette_score(X, clusterer.labels_), 7)\n",
    "\n",
    "                    silhouette_list.append(s)\n",
    "\n",
    "                # The higher (up to 1) the better\n",
    "                #print(silhouette_list)\n",
    "                key = silhouette_list.index(max(silhouette_list))\n",
    "\n",
    "                k = k_list.__getitem__(key)\n",
    "\n",
    "                #print(\"Best silhouette =\", max(silhouette_list), \" for k=\", k)\n",
    "\n",
    "                return max(silhouette_list),k\n",
    "            \n",
    "            \n",
    "            num_clust,k=compare_S_([2,3,4,5,6,7,8,9,10,11,12,13,14,15],df)\n",
    "            silh_score.loc[name]['silhouette_score']=num_clust\n",
    "            silh_score.loc[name]['K']=k\n",
    "            \n",
    "        \n",
    "       \n",
    "           \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(silh_score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_acc=silh_score['silhouette_score'].max()\n",
    "    row=silh_score.loc[silh_score['silhouette_score']==max_acc]\n",
    "    \n",
    "    model_name=row.index.values\n",
    "    print(\"Final Modell\",model_name[0])\n",
    "    \n",
    "    if model_name[0]=='KMeans':\n",
    "        kmeans = KMeans(n_clusters = row['K'].item(), init = 'k-means++', random_state = 12)\n",
    "        y_kmeans = kmeans.fit_predict(df)\n",
    "        plt.scatter(df.iloc[:, 0], df.iloc[:,1],c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "        centers = kmeans.cluster_centers_\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "        labels=kmeans.labels_\n",
    "        clusters=pd.concat([copy_train, pd.DataFrame({'cluster':labels})], axis=1)\n",
    "        clusters.to_csv('project_Kmeans.csv',index=False)\n",
    "        filename = 'finalized_model.sav'\n",
    "        pickle.dump(kmeans, open(filename, 'wb'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#Final Model is Kmeans\\n\")\n",
    "        f.write('model=pickle.load(\"'+filename+'\")\\n')\n",
    "        f.close()\n",
    "        #print(clusters.head())\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif model_name[0]=='GaussianMixture':\n",
    "        sklearn_pca = PCA(n_components =2)\n",
    "        Y_sklearn = sklearn_pca.fit_transform(df)\n",
    "        gmm = GaussianMixture(n_components=row['K'].item(), covariance_type='full').fit(Y_sklearn)\n",
    "        prediction_gmm = gmm.predict(Y_sklearn)\n",
    "        probs = gmm.predict_proba(Y_sklearn)\n",
    "\n",
    "        centers = np.zeros((3,2))\n",
    "        for i in range(3):\n",
    "            density = mvn(cov=gmm.covariances_[i], mean=gmm.means_[i]).logpdf(Y_sklearn)\n",
    "            centers[i, :] = Y_sklearn[np.argmax(density)]\n",
    "\n",
    "        plt.figure(figsize = (10,8))\n",
    "        plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction_gmm ,s=50, cmap='viridis')\n",
    "        plt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);\n",
    "        clusters=pd.concat([df, pd.DataFrame({'cluster':prediction_gmm})], axis=1)\n",
    "        filename = 'finalized_model.sav'\n",
    "        pickle.dump(gmm, open(filename, 'wb'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#Final Model is GaussianMixture+\\n\")\n",
    "        f.write('model=pickle.load(\"'+filename+'\")\\n')\n",
    "        f.close()\n",
    "        #clusters.head()\n",
    "        \n",
    "        \n",
    "    elif model_name[0]=='DBSCAN':\n",
    "        \n",
    "        \n",
    "\n",
    "        db = DBSCAN(eps=0.5, min_samples=10)\n",
    "        db.fit(df)\n",
    "        y_pred = db.fit_predict(df)\n",
    "        plt.scatter(df.iloc[:,0], df.iloc[:,1],c=y_pred, cmap='Paired')\n",
    "        plt.title(\"DBSCAN\")\n",
    "        clusters=pd.concat([df, pd.DataFrame({'cluster':y_pred})], axis=1)\n",
    "        #clusters.head()\n",
    "        filename = 'finalized_model.sav'\n",
    "        pickle.dump(db, open(filename, 'wb'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#Final Model is DBSCAN+\\n\")\n",
    "        f.write('model=pickle.load(\"'+filename+'\")\\n')\n",
    "        f.close()\n",
    "        \n",
    "    elif model_name[0]=='AgglomerativeClustering':\n",
    "        \n",
    "        \n",
    "        hier = AgglomerativeClustering(n_clusters=row['K'].item())\n",
    "        y_pred1 = hier.fit_predict(df)\n",
    "        plt.scatter(df.iloc[:,0], df.iloc[:,1],c=y_pred1, cmap='Paired')\n",
    "        plt.title(\"Agglomerative Clustering\")\n",
    "        clusters=pd.concat([df, pd.DataFrame({'cluster':y_pred1})], axis=1)\n",
    "        filename = 'finalized_model.sav'\n",
    "        pickle.dump(hier, open(filename, 'wb'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#Final Model is Agglomerative Clustering+\\n\")\n",
    "        f.write('model=pickle.load(\"'+filename+'\")\\n')\n",
    "        f.close()\n",
    "        #print(clusters.head())\n",
    "        \n",
    "    elif model_name[0]=='SpectralClustering':\n",
    "        sc=SpectralClustering(n_clusters=row['K'].item(),assign_labels=\"discretize\",random_state=0)\n",
    "        y_pred1 = sc.fit_predict(df)\n",
    "        \n",
    "        plt.scatter(df.iloc[:,0], df.iloc[:,1],c=y_pred1, cmap='Paired')\n",
    "        plt.title(\"SpectralClustering\")\n",
    "        clusters=pd.concat([df, pd.DataFrame({'cluster':y_pred1})], axis=1)\n",
    "        filename = 'finalized_model.sav'\n",
    "        pickle.dump(sc, open(filename, 'wb'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#Final Model is SpectralClustering+\\n\")\n",
    "        f.write('model=pickle.load(\"'+filename+'\")\\n')\n",
    "        f.close()\n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start('C:\\\\Users\\\\RAPTOR\\\\Downloads\\\\train.csv','revenue','Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
